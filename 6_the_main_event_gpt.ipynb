{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e934d3",
   "metadata": {},
   "source": [
    "# **6: The Main Event - Building a Transformer (GPT)**\n",
    "\n",
    "In the previous sections, we built up the foundational components of a language model, starting from understanding the data and creating a simple bigram model. Now, we are ready to build the main event: a `Transformer-based language model`, specifically a GPT (Generative Pre-trained Transformer) architecture. This model will be capable of generating coherent and contextually relevant text based on the input it receives. We will go through the architecture, how it works, and how to train it effectively.\n",
    "\n",
    "\n",
    "## **The Limitations of the Bigram Model**\n",
    "\n",
    "Before `Transformers`, we had simpler models like the `Bigram Model` that could only predict the next token based on the previous token. While this is a good starting point, it has significant limitations:\n",
    "\n",
    "- It can only capture relationships between adjacent tokens, which means it cannot understand long-range dependencies in the text.\n",
    "- It does not have the capacity to learn complex patterns or structures in the language, such as grammar, syntax, or semantics.\n",
    "- It is not capable of generating coherent text over longer sequences, as it lacks the ability to maintain context beyond the immediate previous token.\n",
    "- Each prediction is made independently, without considering the broader context of the entire sequence, leading to disjointed and incoherent text generation.\n",
    "\n",
    "Imgine trying to complete a sentence where you can only see the last word you wrote. You would have no idea what you just wrote, and it would be very difficult to write a coherent sentence. This is the problem with the `Bigram Model` - it can only see one token at a time, and it cannot maintain any context or understanding of the overall structure of the text.\n",
    "\n",
    "\n",
    "## **Introducing the Transformer Architecture**\n",
    "\n",
    "The `Transformer` architecture, introduced in the paper \"Attention is All You Need\" by Vaswani et al., revolutionized the field of natural language processing (NLP) by allowing models to capture long-range dependencies and complex patterns in text. The key innovation of the Transformer is the `self-attention mechanism`, which enables the model to weigh the importance of different tokens in the input sequence when making predictions. Its the foundation for many state-of-the-art language models, including GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), and many others. The transfomer superpower is its ability to process and understand sequences of data, such as text, by capturing long-range dependencies and contextual relationships between tokens.\n",
    "\n",
    "**self-attention** allows the model to consider the entire input sequence when making predictions, rather than just the previous token. This means that the model can learn to capture complex patterns and structures in the language, such as grammar, syntax, and semantics, which are essential for generating coherent and contextually relevant text. It allows every token in the input sequence to attend to every other token, enabling the model to capture relationships between tokens regardless of their distance in the sequence. This is particularly important for understanding and generating natural language, where the meaning of a word can depend on the context provided by other words in the sentence or paragraph. for example, in the sentence \"The cat sat on the mat,\" the word \"cat\" is related to \"sat\" and \"mat,\" and the self-attention mechanism allows the model to capture these relationships effectively. The `Transformer` architecture consists of an encoder and a decoder, but for language modeling tasks like GPT, we typically use only the decoder part of the architecture. The decoder is responsible for generating text based on the input it receives, and it uses self-attention to capture the relationships between tokens in the input sequence. The decoder is made up of multiple layers of self-attention and feed-forward neural networks, which allow it to learn complex patterns and structures in the language.\n",
    "\n",
    "\n",
    "## **The Problem with Attention and the Solution: Flash Attention**\n",
    "\n",
    "While the standard self-attention mechanism is powerful, it can be computationally expensive, especially for long sequences. The attention mechanism requires calculating the attention scores for every pair of tokens in the input sequence, which can lead to a quadratic increase in computational complexity as the sequence length increases. This can make it difficult to train large models on long sequences of text. It needs to create a large `(sequence_length, sequence_length)` attention matrix, to store attention scores for every pair of tokens. This can lead to memory issues and slow down training, especially for long sequences. For a sequence of length `n`, the attention mechanism requires `O(n^2)` computations, which can become prohibitive as `n` increases. This matrix is slow to read from and write to the `GPU's` main memory (HBM - High Bandwidth Memory), which can significantly slow down training and inference. \n",
    "\n",
    "\n",
    "## **Flash Attention: A More Efficient Attention Mechanism**\n",
    "\n",
    "To address this issue, researchers have developed a more efficient attention mechanism called `Flash Attention`. Flash Attention is designed to reduce the memory and computational overhead of the standard attention mechanism by using a more efficient algorithm for computing attention scores. It achieves this by using a technique called `sparse attention`, which allows the model to focus on a subset of tokens in the input sequence when calculating attention scores, rather than considering all pairs of tokens. This can significantly reduce the computational complexity and memory requirements of the attention mechanism, allowing for faster training and inference, especially for long sequences of text. Flash Attention is particularly beneficial for training large language models like `GPT`, as it allows them to process longer sequences of text without running into memory issues or slowdowns.\n",
    "\n",
    "It was introduced in the paper \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\" by Tri Dao et al. The key idea behind Flash Attention is to compute attention scores in a more efficient way that reduces memory usage and computational overhead, allowing for faster training and inference of large language models. By using Flash Attention, we can train larger models on longer sequences of text without running into memory issues or slowdowns, making it an important advancement in the field of natural language processing. Instead of storing all attention scores in memory, Flash Attention uses a clever trick: it computes the attention output in chunks, keeping intermediate results only in the `GPU's` super-fast cache (SRAM - Static Random Access Memory). \n",
    "\n",
    "Think of it this way: Instead of writing a huge intermediate report (the attention matrix) to a slow hard drive (HBM), Flash Attention does all its calculations in the `CPU's` super-fast cache (SRAM), which is much faster to read from and write to in one go. This results in a massive speedup and uses much less memory, often 10-20x faster and requiring far less memory for long sequences. This makes it possible to train larger models on longer sequences of text without running into memory issues or slowdowns, making it an important advancement in the field of natural language processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9b896",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020a9e6",
   "metadata": {},
   "source": [
    "## **Implementing Attention the Modern Way**:\n",
    "\n",
    "We don't need to implement Flash Attention from scratch, as it is already available in libraries like `xformers` and `triton`. These libraries provide efficient implementations of Flash Attention that we can easily integrate into our GPT model. By using these libraries, we can take advantage of the performance benefits of Flash Attention without having to worry about the underlying implementation details. This allows us to focus on building and training our GPT model, while still benefiting from the efficiency of Flash Attention for handling long sequences of text.\n",
    "\n",
    "PyTorch has also introduced native support for Flash Attention in its `torch.nn` module, making it even easier to use this efficient attention mechanism in our models. By simply using the appropriate attention layer provided by PyTorch, we can leverage the benefits of Flash Attention without needing to install additional libraries or write custom code. This integration allows us to build and train our GPT model with improved efficiency and performance, especially when dealing with long sequences of text.\n",
    "\n",
    "The function signatiure is simple:\n",
    "\n",
    "`F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)`\n",
    "\n",
    "This function computes the scaled dot-product attention, which is the core operation in the attention mechanism. The `query`, `key`, and `value` tensors are the inputs to the attention mechanism, and the function computes the attention output based on these inputs. The `attn_mask` can be used to mask out certain positions in the input sequence, while `dropout_p` specifies the dropout probability for regularization. \n",
    "\n",
    "**The Casual Mask**:\n",
    "\n",
    "The `is_causal` flag indicates whether to apply a causal mask, which is typically used in autoregressive models like GPT to prevent attending to future tokens. By using this function, we can efficiently compute attention scores and outputs while benefiting from the performance advantages of Flash Attention.\n",
    "\n",
    "This is much simpler than implementing the attention mechanism from scratch, and it allows us to take advantage of the optimized implementation provided by PyTorch, which is designed to be efficient and fast, especially for long sequences of text. By using this function, we can easily integrate Flash Attention into our GPT model and benefit from its efficiency without having to worry about the underlying implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c74e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# set device to GPU if available else mps (Apple Silicon) else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61495aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape from attention head: torch.Size([2, 10, 16])\n",
      "Input shape: torch.Size([2, 10, 64]), Output shape: torch.Size([2, 10, 16])\n",
      "Head successfully computed self-attention!\n"
     ]
    }
   ],
   "source": [
    "# The Attention Head\n",
    "class Head(nn.Module):\n",
    "    \"\"\"A single head of self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, head_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Each head has its own set of linear layers to compute queries, keys, and values\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, seq_length, n_embd)\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # compute query, key, value matrices\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # Use PyTorch's optimized sclaled dot-product attention function\n",
    "        # This function computes the attention scores and applies them to the value vectors efficiently\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v, \n",
    "                                                     is_causal=True, # ensure that the model cannot attend to future tokens\n",
    "                                                     dropout_p=self.dropout.p if self.training else 0.0) # apply dropout during training\n",
    "        return attn_output # (B, T, head_size)\n",
    "    \n",
    "\n",
    "# Test the attention head\n",
    "n_embd = 64 # embedding dimension\n",
    "head_size = 16 # size of each attention head\n",
    "head = Head(n_embd, head_size).to(device)\n",
    "\n",
    "# Create a test input tensor of shape (batch_size, seq_length, n_embd)\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "x = torch.randn(batch_size, seq_length, n_embd).to(device)\n",
    "output = head(x)\n",
    "print(f\"Output shape from attention head: {output.shape}\") # should be (batch_size, seq_length, head_size)\n",
    "print(f\"Input shape: {x.shape}, Output shape: {output.shape}\")\n",
    "print(f\"Head successfully computed self-attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa9466e",
   "metadata": {},
   "source": [
    "## **Building the Full Transformer Block**\n",
    "\n",
    "The full `Transformer` block consists of multiple layers of self-attention and feed-forward neural networks. Each layer includes a multi-head self-attention mechanism followed by a position-wise feed-forward network. The multi-head self-attention allows the model to attend to different parts of the input sequence simultaneously, while the feed-forward network helps to capture complex patterns and relationships in the data. By stacking multiple layers of these components, we can create a powerful language model that can generate coherent and contextually relevant text based on the input it receives. The architecture of the Transformer block can be summarized as follows:\n",
    "\n",
    "1. **Multi-Head Self-Attention**: This component allows the model to attend to different parts of the input sequence simultaneously. It consists of multiple attention heads, each of which computes attention scores and outputs for a different representation of the input data. The outputs from all attention heads are then concatenated and passed through a linear layer to produce the final output of the self-attention mechanism. One head might focus on grammatical relationships, another on semantic meaning, and another on long-range dependencies. By concatenating the outputs of multiple heads, we get a richer representation. \n",
    "\n",
    "2. **Position-Wise Feed-Forward Network**: This component consists of two linear layers with a `ReLU` activation function in between. It is applied to each position in the input sequence independently, allowing the model to capture complex patterns and relationships in the data.\n",
    "\n",
    "3. **Layer Normalization and Residual Connections**: Each layer of the Transformer block includes layer normalization and residual connections to help stabilize training and improve the flow of gradients through the network. The output of the multi-head self-attention is added to the input of the feed-forward network, and the output of the feed-forward network is added back to the input of the multi-head self-attention, creating a residual connection that helps to prevent vanishing gradients and allows for deeper networks.\n",
    "\n",
    "4. **Stacking Multiple Layers**: By stacking multiple layers of these components, we can create a powerful language model that can capture complex patterns and relationships in the data, allowing it to generate coherent and contextually relevant text based on the input it receives.\n",
    "\n",
    "5. **Output Layer**: Finally, the output from the last Transformer block is passed through a linear layer to produce logits for each token in the vocabulary, which can then be converted to probabilities using the softmax function for text generation.\n",
    "\n",
    "6. **Positional Encoding**: Since the Transformer architecture does not have any inherent notion of the order of tokens in the input sequence, we need to add positional encoding to the input embeddings to provide the model with information about the position of each token in the sequence. This allows the model to capture the sequential nature of language and generate coherent text based on the order of tokens.\n",
    "\n",
    "7. **Training the Model**: To train the Transformer-based language model, we typically use a large corpus of text data and optimize the model's parameters using a loss function such as cross-entropy loss. The model learns to predict the next token in the sequence based on the previous tokens, allowing it to generate coherent and contextually relevant text over time.\n",
    "\n",
    "8. **Autoregressive Generation**: Once the model is trained, we can use it to generate text by feeding in an initial token and repeatedly predicting the next token until we reach a desired length of generated text. This process is known as `autoregressive generation`, where the model generates one token at a time based on the previously generated tokens, allowing it to create coherent and contextually relevant text over time.\n",
    "\n",
    "9.  **Sampling Strategies**: When generating text, we can use different sampling strategies to control the diversity and creativity of the generated text. Common strategies include \n",
    "- `greedy sampling` (choosing the token with the highest probability), \n",
    "- `top-k sampling` (choosing from the top k most probable tokens), and \n",
    "- `nucleus sampling` (choosing from the smallest set of tokens whose cumulative probability exceeds a certain threshold). These strategies allow us to balance between generating coherent text and introducing some level of randomness and creativity in the output.\n",
    "\n",
    "10. **Fine-Tuning and Transfer Learning**: After training the base Transformer model on a large corpus of text, we can fine-tune it on specific tasks or domains by continuing to train the model on a smaller, task-specific dataset. This allows the model to adapt its knowledge to the specific requirements of the task, such as sentiment analysis, question answering, or machine translation, while still leveraging the general language understanding it has learned from the larger corpus.\n",
    "\n",
    "11. **Evaluation and Metrics**: To evaluate the performance of the Transformer-based language model, we can use various metrics such as perplexity, BLEU score, ROUGE score, or human evaluation. These metrics help us assess the quality of the generated text and compare it to reference texts or human-generated outputs. By evaluating the model's performance, we can identify areas for improvement and further refine the architecture or training process to achieve better results.\n",
    "\n",
    "12. **Deployment and Applications**: Once we have a trained Transformer-based language model, we can deploy it in various applications such as chatbots, virtual assistants, content generation, and more. The model can be integrated into applications to provide natural language understanding and generation capabilities, allowing for more interactive and engaging user experiences. By leveraging the power of the Transformer architecture, we can create sophisticated language models that can understand and generate human-like text for a wide range of applications.\n",
    "\n",
    "\n",
    "\n",
    "The complete `Transformer` Block combines all of these:\n",
    "- `Multi-Head Attention` -> `Residual Connection & LayerNorm` -> `FeedForward Network` -> `Residual Connection & LayerNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d79a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block input shape: torch.Size([2, 10, 64])\n",
      "Block output shape: torch.Size([2, 10, 64])\n",
      "Transformer Block successfully created!\n"
     ]
    }
   ],
   "source": [
    "# Multi-Head Attention: Run multiple heads in parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple attention heads running in parallel.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % num_heads == 0, \"n_embd must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = n_embd // num_heads\n",
    "        self.n_embd = n_embd\n",
    "        \n",
    "        # Create multiple heads\n",
    "        self.heads = nn.ModuleList([Head(n_embd, self.head_size, dropout) for _ in range(num_heads)])\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Run each head in parallel and concatenate\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # Apply output projection\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# FeedForward Network: A simple MLP\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple 2-layer MLP with GELU activation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expand by 4x\n",
    "            nn.GELU(),                       # GELU is smoother than ReLU\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# The Transformer Block: The complete building block\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication (attention) followed by computation (feedforward).\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Multi-head self-attention\n",
    "        self.sa = MultiHeadAttention(n_embd, num_heads, dropout)\n",
    "        # Feedforward network\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        # Layer normalization\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Attention with residual connection and layer norm\n",
    "        x = x + self.sa(self.ln1(x))  # Residual connection\n",
    "        \n",
    "        # Feedforward with residual connection and layer norm\n",
    "        x = x + self.ffwd(self.ln2(x))  # Residual connection\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the Block\n",
    "n_embd = 64\n",
    "num_heads = 4\n",
    "block = Block(n_embd, num_heads).to(device)\n",
    "\n",
    "test_input = torch.randn(2, 10, n_embd).to(device)\n",
    "output = block(test_input)\n",
    "print(f\"Block input shape: {test_input.shape}\")\n",
    "print(f\"Block output shape: {output.shape}\")\n",
    "print(f\"Transformer Block successfully created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66d9ad",
   "metadata": {},
   "source": [
    "## **The Final GPT Model**\n",
    "\n",
    "Now we can assemble the complete `GPT` (Generative Pre-trained Transformer) model. The architecture consists of:\n",
    "\n",
    "1. `Token Embedding Layer`: Converts token indices (integers) into dense vectors, just like the `Bigram model`. Each token in the vocabulary gets mapped to a learnable embedding vector.\n",
    "\n",
    "2. `Positional Embedding Layer`: Since attention treats all tokens equally, we need to give the model a sense of token order. Positional embeddings encode the position of each token in the sequence, allowing the model to understand \"first word\", \"second word\", etc.\n",
    "\n",
    "3. `Stack of Transformer Blocks`: Multiple blocks (typically 6-12 for small models, 96+ for large models) stacked on top of each other. Each block refines the understanding of the sequence.\n",
    "\n",
    "4. `Final LayerNorm`: Normalizes the final representations before the output layer.\n",
    "\n",
    "5. `Output Linear Layer`: Maps the final hidden representations back to vocabulary logits (scores for each token in the vocabulary).\n",
    "\n",
    "\n",
    "The `generate` method is almost identical to the Bigram model, we still sample tokens one at a time, but now each prediction benefits from the full context of all previous tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd869dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTLanguageModel class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT model: a stack of transformer blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embd, block_size, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Each token directly reads off the logits from the embedding table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # Language model head\n",
    "        \n",
    "        # Better initialization\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
    "        x = self.blocks(x)  # (B, T, n_embd)\n",
    "        x = self.ln_f(x)  # (B, T, n_embd)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate new tokens given a context.\n",
    "        \n",
    "        Args:\n",
    "            idx: (B, T) array of indices in the current context\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Controls randomness (higher = more random)\n",
    "            top_k: Only sample from top k most likely tokens\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:] if idx.shape[1] >= self.block_size else idx\n",
    "            \n",
    "            # Get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] / temperature  # (B, C)\n",
    "            \n",
    "            # Optionally apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        \n",
    "        self.train()\n",
    "        return idx\n",
    "\n",
    "print(\"GPTLanguageModel class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a229503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36\n",
      "Characters: \n",
      " -.ADLMNTabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 65  # Character-level vocabulary (for simplicity)\n",
    "block_size = 256  # Maximum context length\n",
    "n_embd = 384  # Embedding dimension\n",
    "num_heads = 6  # Number of attention heads\n",
    "num_layers = 6  # Number of transformer blocks\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "# Create a simple text dataset for demonstration\n",
    "# In practice, you'd load a real dataset like Shakespeare, Wikipedia, etc.\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "The dog barks at the fox. The fox runs away quickly.\n",
    "Machine learning is fascinating. Deep learning models can understand language.\n",
    "Transformers are powerful architectures. Attention mechanisms enable long-range dependencies.\n",
    "Natural language processing has advanced rapidly. Large language models can generate coherent text.\n",
    "Artificial intelligence continues to evolve. Neural networks learn complex patterns.\n",
    "\"\"\"\n",
    "\n",
    "# Create character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5aa4256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset downloaded.\n",
      "\n",
      "Vocabulary size: 65\n",
      "Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "\n",
    "# Download the tiny shakespeare dataset\n",
    "# This is a small dataset of Shakespeare's works, which is often used for character-level language modeling.\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "response = request.urlopen(url)\n",
    "text = response.read().decode('utf-8')\n",
    "print(\"Dataset downloaded.\\n\")\n",
    "\n",
    "\n",
    "# Create character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1975854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 1003854\n",
      "Val data length: 111540\n"
     ]
    }
   ],
   "source": [
    "# Create character-to-index mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Encode the text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Split into train and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train data length: {len(train_data)}\")\n",
    "print(f\"Val data length: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53abaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n",
      "Total parameters: 10,788,929\n",
      "Trainable parameters: 10,788,929\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data.\"\"\"\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Instantiate the model\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model created!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e16b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Step 0: train loss 4.2518, val loss 4.2430\n",
      "Step 500: train loss 2.0092, val loss 2.0920\n",
      "Step 1000: train loss 1.5070, val loss 1.6955\n",
      "Step 1500: train loss 1.3326, val loss 1.5652\n",
      "Step 2000: train loss 1.2286, val loss 1.5069\n",
      "Step 2500: train loss 1.1483, val loss 1.5020\n",
      "Step 3000: train loss 1.0672, val loss 1.5099\n",
      "Step 3500: train loss 0.9789, val loss 1.5403\n",
      "Step 4000: train loss 0.8841, val loss 1.5961\n",
      "Step 4500: train loss 0.7804, val loss 1.6683\n",
      "Step 4999: train loss 0.6728, val loss 1.7579\n",
      "\n",
      "Training completed!\n",
      "\n",
      "==================================================\n",
      "Generating text with GPT:\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimate loss on train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for iter_num in range(max_iters):\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Generate text\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generating text with GPT:\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e696b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pale me believe me.\n",
      "\n",
      "LEONTES:\n",
      "How! then, Camillo!\n",
      "Though me our bastard; my thrice souls have been with\n",
      "A shepherd's death, my lord.\n",
      "\n",
      "PAULINA:\n",
      "I am assist for this appointed tears,\n",
      "Poor surprised with\n",
      "\n",
      "==================================================\n",
      "Notice how the GPT model produces much more coherent text\n",
      "compared to a Bigram model, thanks to self-attention!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Start with a context\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = model.generate(context, max_new_tokens=200, temperature=0.8, top_k=50)\n",
    "generated_text = decode(generated[0].tolist())\n",
    "print(generated_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Notice how the GPT model produces much more coherent text\")\n",
    "print(\"compared to a Bigram model, thanks to self-attention!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af77f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55f648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4977489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a256a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f687722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed71e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4873fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf417c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1a0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d60da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681baf65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bb7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79415ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-llm-majors (3.10.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
