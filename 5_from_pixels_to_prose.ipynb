{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb16da1b",
   "metadata": {},
   "source": [
    "# **5: From Pixels to Prose - Your First Language Model**\n",
    "\n",
    "In the previous sections, we explored the fascinating world of `computer vision`, where machines learn to see and interpret images. Now, we are about to embark on a new journey into the realm of `natural language processing (NLP)`, where machines learn to understand and generate human language. In this section, we will build our first language model, a powerful tool that can generate coherent and meaningful text based on the patterns it has learned from a large corpus of data.\n",
    "\n",
    "Deep learning isn't limited to just images and vision; it has revolutionized the way we process and understand language. Language models are at the heart of many applications, from chatbots and virtual assistants to machine translation and content generation, where models learn to understand and generate human language. \n",
    "\n",
    "In this section, we will explore the architecture of language models, starting with the simplest possible language model: the `Bigram Model`. This model predicts the next word in a sequence based on the previous word, allowing us to generate text one word at a time. It has no memory of previous words, making it a simple yet powerful tool for understanding the basics of language modeling. Despite its simplicity, the `Biagram Model` will teach us the fundamental concepts of language modeling, such as tokenization, probability estimation, batch creation, and autoregressive text generation.\n",
    "\n",
    "\n",
    "\n",
    "## **The Data and the Tokenizer**\n",
    "\n",
    "To build our language model, we need a dataset to train on. For this example, we will use a simple text dataset, such as a collection of sentences or a small corpus of text. The first step in processing this data is to tokenize it, which means breaking down the text into smaller units called tokens. Tokens can be words, subwords, or even characters, depending on the level of granularity we want to achieve.\n",
    "\n",
    "Before we can train our model, we need to convert our text data into a format that the model can understand. This involves creating a vocabulary of unique tokens and mapping each token to a unique integer index. This process is known as `tokenization`.\n",
    "\n",
    "Since our model will be a `Bigram Model`, we will focus on tokenizing the text at the `character-level tokenizer` which breaks down the text into individual characters. This allows us to capture the structure of the language at a very fine-grained level, which can be useful for generating text that is more coherent and natural.\n",
    "\n",
    "For example, if we have the sentence \"Hello world\", our character-level tokenizer would break it down into the following tokens: `H`, `e`, `l`, `o`, ` ` (space), `w`, `r`, `d`. Each of these tokens would then be assigned a unique integer index in our vocabulary.\n",
    "\n",
    "i.e;\n",
    "\n",
    "- `a` -> 0\n",
    "- `b` -> 1\n",
    "- `c` -> 2\n",
    "- `d` -> 3\n",
    "- `e` -> 4\n",
    "- `f` -> 5\n",
    "- And so on...\n",
    "\n",
    "\n",
    "## **The Components of our Tokenizer**\n",
    "\n",
    "1. `Vocabulary Creation`: We will create a vocabulary of unique tokens from our dataset. This involves iterating through the text and collecting all the unique characters (or words) that appear in the dataset.\n",
    "\n",
    "2. `stoi (string to index)`: We will create a mapping from each token in our vocabulary to a unique integer index. This allows us to convert our text data into numerical format that can be fed into the model.\n",
    "\n",
    "3. `itos (index to string)`: We will also create a mapping from integer indices back to their corresponding tokens. This is useful for converting the model's output back into human-readable text.\n",
    "\n",
    "4. `Encoding and Decoding Functions`: We will implement functions to encode text into sequences of integer indices and to decode sequences of indices back into text. This will allow us to easily convert between the raw text and the numerical format required for training our model.\n",
    "\n",
    "With these components in place, we will be able to preprocess our text data and prepare it for training our `Bigram Model`. The tokenizer will play a crucial role in ensuring that our model can effectively learn the patterns and structure of the language from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9514f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset downloaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import urllib.request as request\n",
    "\n",
    "# Download the tiny shakespeare dataset\n",
    "# This is a small dataset of Shakespeare's works, which is often used for character-level language modeling.\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "response = request.urlopen(url)\n",
    "data = response.read().decode('utf-8')\n",
    "print(\"Dataset downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886ccfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394 characters\n",
      "First 250 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length: {len(data)} characters\")\n",
    "print(f\"First 250 characters:\\n{data[:250]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03ab5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary of unique characters in the dataset\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "print(f\"Vocabulary size: {vocab_size} unique characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc0a3f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoi mapping: [('\\n', 0), (' ', 1), ('!', 2), ('$', 3), ('&', 4), (\"'\", 5), (',', 6), ('-', 7), ('.', 8), ('3', 9)]\n",
      "itos mapping: [(0, '\\n'), (1, ' '), (2, '!'), (3, '$'), (4, '&'), (5, \"'\"), (6, ','), (7, '-'), (8, '.'), (9, '3')]\n",
      "\n",
      "Example itos mapping: 1 -> ' ', 2 -> '!', 3 -> '$'\n",
      "Example stoi mapping: ' ' -> 1, 'a' -> 39, 'b' -> 40\n",
      "\n",
      "Total unique characters: 65\n",
      "Total unique characters (itos): 65\n"
     ]
    }
   ],
   "source": [
    "# Create the mappings from characters to integers and vice versa\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # string to integer\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # integer to string\n",
    "\n",
    "print(f\"stoi mapping: {list(stoi.items())[:10]}\")  # Print first 10 mappings\n",
    "print(f\"itos mapping: {list(itos.items())[:10]}\\n\")  # Print first 10 mappings\n",
    "print(f\"Example itos mapping: 1 -> '{itos[1]}', 2 -> '{itos[2]}', 3 -> '{itos[3]}'\")\n",
    "print(f\"Example stoi mapping: ' ' -> {stoi[' ']}, 'a' -> {stoi['a']}, 'b' -> {stoi['b']}\\n\")\n",
    "print(f\"Total unique characters: {len(stoi)}\")\n",
    "print(f\"Total unique characters (itos): {len(itos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae36ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example stoi mapping: 'a' -> 39\n",
      "Example stoi mapping: 'H' -> 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example stoi mapping: 'a' -> {stoi.get('a', 'N/A')}\")\n",
    "print(f\"Example stoi mapping: 'H' -> {stoi.get('H', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d50a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test string: Hello, World!\n",
      "Encoded: [20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "Decoded: Hello, World!\n",
      "Round-trip successful: 'Hello, World!' -> [20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2] -> 'Hello, World!'\n"
     ]
    }
   ],
   "source": [
    "# Define encode and decode functions\n",
    "def encode(s):\n",
    "    \"encoder takes a string and outputs a list of integers\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    \"decoder takes a list of integers and outputs a string\"\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test the encode and decode functions\n",
    "test_string = \"Hello, World!\"\n",
    "encoded = encode(test_string)\n",
    "decoded = decode(encoded)\n",
    "\n",
    "print(f\"Test string: {test_string}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "assert test_string == decoded, \"Decoded string does not match original\"\n",
    "print(f\"Round-trip successful: '{test_string}' -> {encoded} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c34947",
   "metadata": {},
   "source": [
    "## **Creating Batches of Data for Language Models**\n",
    "\n",
    "Unlike in image classification, where each data point is an independent image (one image corresponds to one label), language modeling involves sequences of tokens that are interdependent. The model needs to learn the relationships and patterns in the text, which means that we need to create batches of data that capture these dependencies.\n",
    "\n",
    "\n",
    "### **The Input-Output Relationship in Language Models**\n",
    "\n",
    "For a language model, the input is a sequence of tokens (e.g., characters or words), and the output is the next token in the sequence. For example, if we have the input sequence \"H\", \"e\", \"l\", \"l\", \"o\", the model should learn to predict the next token, which is \" \" (space). This means that our training data will consist of pairs of input sequences and their corresponding target tokens.\n",
    "\n",
    "- For example, if we have the text \"Hello world\", we can create training pairs like this:\n",
    "- Input: \"H\" -> Target: \"e\"\n",
    "- Input: \"He\" -> Target: \"l\"\n",
    "- Input: \"Hel\" -> Target: \"l\"\n",
    "- Input: \"Hell\" -> Target: \"o\"\n",
    "- Input: \"Hello\" -> Target: \" \" (space)\n",
    "- Input: \"Hello \" -> Target: \"w\"\n",
    "- And so on... The same chunk shifted by one character to the right, creating a new input-target pair each time.\n",
    "\n",
    "For every character in the input sequence, we will have a corresponding target character that the model needs to learn to predict. This creates a rich dataset of input-target pairs that the model can use to learn the patterns and structure of the language.\n",
    "\n",
    "### **Block Size and Context Window**\n",
    "\n",
    "The `block size` (or `context window`) is a crucial hyperparameter in language modeling. It determines how many previous tokens the model can see when making a prediction. For example, if we set a block size of 5, the model will only be able to see the last 5 tokens when predicting the next token. This means that the model will learn to capture dependencies and patterns within that context window.\n",
    "\n",
    "For a `Bigram Model`, the block size is effectively 1, since the model only looks at the previous token to predict the next one. However, as we move to more complex models like `Trigram Models` or `Transformer-based Models`, we can increase the block size to capture longer-range dependencies in the text.\n",
    "\n",
    "\n",
    "### **Creating Batches of Data**\n",
    "\n",
    "To create batches of data for training our language model, we will implement a function that takes in the raw text data and generates batches of input-target pairs based on the specified block size. This function will randomly sample sequences from the dataset and create corresponding target tokens for each input sequence. The batch shape will be `(batch_size, block_size)`, where\n",
    "\n",
    "- `batch_size` is the number of sequences in each batch and \n",
    "- `block_size` is the length of each input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a609a869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available or MPS if on Apple Silicon, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b07f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([1115394])\n",
      "First 10 integers in data tensor: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47]\n",
      "Data type of data tensor: torch.int64\n",
      "Total tokens in dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "# Convert the entire text dataset into a tensor of integers\n",
    "data_tensor = torch.tensor(encode(data), dtype=torch.long)\n",
    "print(f\"Data tensor shape: {data_tensor.shape}\")\n",
    "print(f\"First 10 integers in data tensor: {data_tensor[:10].tolist()}\")\n",
    "print(f\"Data type of data tensor: {data_tensor.dtype}\")\n",
    "print(f\"Total tokens in dataset: {len(data_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6de913ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([1003854])\n",
      "Validation data shape: torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# Split into train and validation sets (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(data_tensor))\n",
    "train_data = data_tensor[:train_size]\n",
    "val_data = data_tensor[train_size:] \n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "621e73c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch example:\n",
      "x_batch shape: torch.Size([4, 8])\n",
      "y_batch shape: torch.Size([4, 8])\n",
      "\n",
      "First example:\n",
      "Input (x[0]): [47, 52, 58, 43, 56, 54, 56, 43]\n",
      "Target (y[0]): [52, 58, 43, 56, 54, 56, 43, 58]\n",
      "\n",
      "Decoded first example:\n",
      "Input (x[0]): 'interpre'\n",
      "Target (y[0]): 'nterpret'\n",
      "\n",
      "Batch details:\n",
      "x_batch:\n",
      "tensor([[47, 52, 58, 43, 56, 54, 56, 43],\n",
      "        [52,  1, 58, 46, 47, 57,  1, 49],\n",
      "        [43, 39, 60, 43, 52,  1, 44, 53],\n",
      "        [13, 63,  6,  1, 46, 39, 52, 42]])\n",
      "y_batch:\n",
      "tensor([[52, 58, 43, 56, 54, 56, 43, 58],\n",
      "        [ 1, 58, 46, 47, 57,  1, 49, 47],\n",
      "        [39, 60, 43, 52,  1, 44, 53, 56],\n",
      "        [63,  6,  1, 46, 39, 52, 42,  1]])\n"
     ]
    }
   ],
   "source": [
    "# Define batch creation function\n",
    "def get_batch(split, batch_size=4, block_size=8):\n",
    "    \"\"\"\n",
    "    Generate a batch of data for training or validation.\n",
    "    \n",
    "    Args:\n",
    "        :param split: 'train' or 'val' to specify which dataset to use\n",
    "        :param batch_size: Number of sequences in the batch\n",
    "        :param block_size: Length of each sequence (context length)\n",
    "    \n",
    "    Returns:\n",
    "        x: Tensor of shape (batch_size, block_size) containing input sequences\n",
    "        y: Tensor of shape (batch_size, block_size) containing target sequences\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Randomly select starting indices for the batch\n",
    "    # We subtract block_size to ensure we have enough characters for the target sequence\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Create input (x) and target (y) tensors\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Test the get_batch function\n",
    "x_batch, y_batch = get_batch('train', batch_size=4, block_size=8)\n",
    "print(f\"\\nBatch example:\")\n",
    "print(f\"x_batch shape: {x_batch.shape}\")\n",
    "print(f\"y_batch shape: {y_batch.shape}\")\n",
    "\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"Input (x[0]): {x_batch[0].tolist()}\")\n",
    "print(f\"Target (y[0]): {y_batch[0].tolist()}\")\n",
    "\n",
    "print(f\"\\nDecoded first example:\")\n",
    "print(f\"Input (x[0]): '{decode(x_batch[0].tolist())}'\")\n",
    "print(f\"Target (y[0]): '{decode(y_batch[0].tolist())}'\")\n",
    "\n",
    "print(f\"\\nBatch details:\")\n",
    "print(f\"x_batch:\\n{x_batch}\")\n",
    "print(f\"y_batch:\\n{y_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f1c97",
   "metadata": {},
   "source": [
    "## **The Biagram Model Architecture**\n",
    "\n",
    "The `Bigram Model` is a simple language model that predicts the next token in a sequence based on the previous token. It is called a \"bigram\" model because it considers pairs of tokens (bigrams) when making predictions.\n",
    "\n",
    "You can think of `nn.Embedding` as a lookup table that maps each token index to a dense vector representation. The input is the index of a character (e.g., the index of `H` in the vocabulary), and the output is a dense vector that represents that character in a continuous space. The model learns to adjust these embeddings during training so that similar characters (or tokens) have similar embeddings, allowing the model to capture semantic relationships between tokens.\n",
    "\n",
    "The `input` to the embedding layer is a batch of token indices, and the output is a batch of corresponding embeddings. For example, if we have a batch of input token indices of shape `(batch_size, block_size)`, the output from the embedding layer will be of shape `(batch_size, block_size, embedding_dim)`, where `embedding_dim` is the size of the dense vector representation for each token.\n",
    "\n",
    "\n",
    "### **How it works**\n",
    "\n",
    "1. The `input` to the model is a batch of token indices, which are passed through the embedding layer to get their corresponding dense vector representations.\n",
    "\n",
    "2. `Embedding Lookup`: The embedding layer takes a vector of size `vocab_size` (the number of unique tokens) containing logits (raw scores) for each possible next token and produces a vector of size `embedding_dim` for each token in the input batch. This is done by looking up the embedding for each token index in the input.\n",
    "\n",
    "3. `Output`: These logits represent the model's prediction for which character (or token) is most likely to come next in the sequence. The model learns to adjust the embeddings and the linear layer's weights during training to improve its predictions over time.\n",
    "\n",
    "\n",
    "### **Autoregressive Generation**\n",
    "\n",
    "Once the model is trained, we can use it to generate text by feeding in an initial token and repeatedly predicting the next token until we reach a desired length of generated text. This process is known as `autoregressive generation`, where the model generates one token at a time based on the previously generated tokens.\n",
    "\n",
    "To generate a new text, we use an `autoregressive` loop`:\n",
    "\n",
    "- Start with a seed character (or prompt)\n",
    "- Feed the seed character into the model and get the predicted next character\n",
    "- Sample from the predicted probabilities to get the next character (using the logits output from the model)\n",
    "- Append the sampled character to the input sequence\n",
    "- Use that new sequence as the input for the next prediction\n",
    "- Repeat this process until we have generated the desired length of text\n",
    "\n",
    "This is called `autoregressive generation` because the model generates each token based on the previously generated tokens, allowing it to create coherent and contextually relevant text over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "899b5d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 65)\n",
      ")\n",
      "\n",
      "Number of parameters: 4225\n",
      "\n",
      "Forward pass test:\n",
      "Input shape: torch.Size([4, 8])\n",
      "Output shape: torch.Size([32, 65])\n",
      "Loss: 4.4530\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Bigram Language Model that predicts the next token based on the current token.\n",
    "    \n",
    "    This model predicts the next character based solely on the previous character.\n",
    "    Despite its simplicity, it can learn to generate coherent text by capturing the statistical relationships between characters in the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token (character) directly reads off the logits for the \n",
    "        # next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Input tensor of shape (batch_size, block_size) containing token indices\n",
    "            targets: Optional target tensor of shape (batch_size, block_size)\n",
    "                    If provided, loss is computed\n",
    "        \n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, block_size, vocab_size)\n",
    "                    Contains logits for each position and each possible next token\n",
    "            loss: Scalar loss value (only if targets provided)\n",
    "        \"\"\"\n",
    "        # idx and targets are both (batch_size, block_size) tensors of integers\n",
    "        \n",
    "        # Get logits for each position in the input\n",
    "        # Shape: (batch_size, block_size, vocab_size)\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape for cross-entropy: (batch_size * block_size, vocab_size) and (batch_size * block_size,)\n",
    "            B, T, C = logits.shape  # Batch size, Time steps, Vocabulary size\n",
    "            logits = logits.view(B*T, C)  # Reshape to (B*T, C)\n",
    "            targets = targets.view(B*T)    # Reshape to (B*T)\n",
    "            \n",
    "            # compute cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens given a starting sequence.\n",
    "        \n",
    "        Args:\n",
    "            idx: Input tensor of shape (batch_size, block_size) containing token indices\n",
    "            max_new_tokens: Number of new tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            idx: Tensor of shape (batch_size, block_size + max_new_tokens) containing the original and generated token indices\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the logits for the current input\n",
    "            logits, _ = self.forward(idx) # Get logits for last position only\n",
    "            \n",
    "            # Focus only on the last time step's logits\n",
    "            logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Sample from the distribution to get the next token index\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Append the sampled token to the input sequence\n",
    "            idx = torch.cat((idx, next_token), dim=1)  # Shape: (batch_size, current_length + 1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "\n",
    "# Instantiate the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "print(f\"Model:\\n{model}\\n\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# test forward pass\n",
    "x, y = get_batch('train', batch_size=4, block_size=8)\n",
    "logits, loss = model(x, y)\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddd94d",
   "metadata": {},
   "source": [
    "### **Training and Generation**\n",
    "\n",
    "To train the `Bigram Model`, we will use the input-target pairs we created earlier. We will feed the input sequences into the model and compute the loss based on the predicted next token and the actual target token. We will then backpropagate the loss and update the model's parameters using an optimizer.\n",
    "\n",
    "- `Forward pass`: We will pass the input sequences through the model to get the predicted logits for the next token.\n",
    "\n",
    "- `Loss computation`: We will compute the loss using a suitable loss function (e.g., cross-entropy loss) that compares the predicted logits with the actual target tokens.\n",
    "\n",
    "- `Backpropagation`: We will backpropagate the loss to compute the gradients of the model's parameters.\n",
    "\n",
    "- `Parameter update`: We will use an optimizer (e.g., Adam) to update the model's parameters based on the computed gradients.\n",
    "\n",
    "- `Repeat` this process for multiple epochs until the model converges and can generate coherent text based on the training data.\n",
    "\n",
    "After training, we can use the model to generate new text by providing a seed character and using the autoregressive generation process described above. This allows us to create new sentences or paragraphs that are similar in style and content to the training data, demonstrating the model's ability to learn and generate human-like language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "130cad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer:\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Step 0: Train loss = 2.4677, Val loss = 2.4931\n",
      "\n",
      "Step 500: Train loss = 2.4532, Val loss = 2.4900\n",
      "\n",
      "Step 1000: Train loss = 2.4518, Val loss = 2.4844\n",
      "\n",
      "Step 1500: Train loss = 2.4589, Val loss = 2.4792\n",
      "\n",
      "Step 2000: Train loss = 2.4618, Val loss = 2.4850\n",
      "\n",
      "Step 2500: Train loss = 2.4528, Val loss = 2.4828\n",
      "\n",
      "Step 3000: Train loss = 2.4531, Val loss = 2.4974\n",
      "\n",
      "Step 3500: Train loss = 2.4585, Val loss = 2.4805\n",
      "\n",
      "Step 4000: Train loss = 2.4503, Val loss = 2.4827\n",
      "\n",
      "Step 4500: Train loss = 2.4645, Val loss = 2.4822\n",
      "\n",
      "Step 4999: Train loss = 2.4642, Val loss = 2.4917\n",
      "\n",
      "Training completed.\n",
      "\n",
      "========================================\n",
      "\n",
      "Generating text...\n",
      "\n",
      "========================================\n",
      "\n",
      "\n",
      "ABoryon'sish ice broe n,\n",
      "T:\n",
      "Asthest AMathe hes s.\n",
      "ILENENDWNomowifre WA:\n",
      "IZANCEO hery bleld h ho ir'sthatrs gacelang hysef winare ofodlprakend! d belthionona cle kerge?\n",
      "\n",
      "f may s ks n'doa te;\n",
      "INo frowof ulld hoongrs ncknd I melt mus\n",
      "Be kerepat YREN: BOLOUS:\n",
      "Sustheakny she are.\n",
      "HIZA:\n",
      "IS:\n",
      "Tolet,\n",
      "Oprenuns;\n",
      "Thon, thit t awher te harenoumeday S:\n",
      "\n",
      "Thesherlit ith thissa pon ce'l ered.\n",
      "AUSA r, ar maurche gikend I:\n",
      "Be' wow an\n",
      "HEYBuncoure ben kd\n",
      "I booo:\n",
      "Seitoithyod be chevelofrtod tlyod,\n",
      "Bulthat:\n",
      "\n",
      "ame; in\n",
      "I\n",
      "\n",
      "========================================\n",
      "\n",
      "Note: The output may seem random or nonsensical.\n",
      "This is expected for such a simple model!\n",
      "The model is learning basic character-level statistics, but it does not have the capacity to learn complex language patterns or long-range dependencies.\n",
      "To generate more coherent text, we would need to use a more sophisticated model architecture (e.g., RNNs, Transformers) and train on a larger dataset for more iterations.\n",
      "This example serves as a starting point to understand how language models work at a fundamental level.\n",
      "Feel free to experiment with the model, increase the training iterations, or try different starting contexts to see how it affects the generated text!\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "print(f\"Optimizer:\\n{optimizer}\")\n",
    "\n",
    "# Training loop\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = {}\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        for split in ['train', 'val']:\n",
    "            losses[split] = []\n",
    "            for _ in range(eval_iters):\n",
    "                xb, yb = get_batch(split, batch_size, block_size)\n",
    "                _, loss = model(xb, yb)\n",
    "                losses[split].append(loss.item())\n",
    "            losses[split] = torch.mean(torch.tensor(losses[split]))\n",
    "            # print(f\"Iter {iter}: {split} loss = {losses[split].item():.4f}\")\n",
    "        print(f\"Step {iter}: Train loss = {losses['train'].item():.4f}, Val loss = {losses['val'].item():.4f}\\n\")\n",
    "        model.train() # Set model back to training mode\n",
    "        \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Clear gradients before backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # backpropagate to compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Generate some text using the trained model\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(\"Generating text...\\n\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# Start with a newline character (or any character you like)\n",
    "context = torch.zeros(1, 1, dtype=torch.long) # Starting with a single token (e.g., newline)\n",
    "generated = model.generate(context, max_new_tokens=500)[0].tolist() # Generate 500 new tokens\n",
    "print(decode(generated))\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(\"Note: The output may seem random or nonsensical.\")\n",
    "print(\"This is expected for such a simple model!\")\n",
    "print(\"The model is learning basic character-level statistics, but it does not have the capacity to learn complex language patterns or long-range dependencies.\")\n",
    "print(\"To generate more coherent text, we would need to use a more sophisticated model architecture (e.g., RNNs, Transformers) and train on a larger dataset for more iterations.\")\n",
    "print(\"This example serves as a starting point to understand how language models work at a fundamental level.\")\n",
    "print(\"Feel free to experiment with the model, increase the training iterations, or try different starting contexts to see how it affects the generated text!\")\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e59f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007613c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756148f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a240498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04703c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586119c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ba608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df15830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd5131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effdd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4802a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-llm-majors (3.10.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
