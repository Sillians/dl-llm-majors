{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c793039",
   "metadata": {},
   "source": [
    "# **2: Your First Model - A Multilayer Perceptron (MLP)**\n",
    "\n",
    "## **Data Loading and Visualization**\n",
    "\n",
    "### The MNIST Dataset\n",
    "\n",
    "`MNIST` is a popular dataset in the machine learning community, consisting of `70,000` grayscale images of handwritten digits `(0-9)`. Each image is `28x28` pixels, and the task is to classify each image into one of the 10 digit classes. The dataset is split into a training set of `60,000` images and a test set of `10,000` images.\n",
    "\n",
    "**torchvision.datasets** \n",
    "\n",
    "PyTorch provides a convenient way to load the MNIST dataset through the `torchvision.datasets` module. You can use the `MNIST` class to download and load the dataset.\n",
    "\n",
    "**DataLoaders** \n",
    "\n",
    "To efficiently load and batch the data during training, we use `DataLoader` from `torch.utils.data`. A `DataLoader` takes a dataset and provides an iterable over the dataset with support for automatic batching, shuffling, and parallel data loading.\n",
    "\n",
    "\n",
    "**transforms.ToTensor()**\n",
    "\n",
    "The `transforms.ToTensor()` function converts a PIL image or a NumPy array into a PyTorch tensor. It also scales the pixel values from the range `[0, 255]` to `[0.0, 1.0]`, which is important for training neural networks as it helps with convergence. When you apply `transforms.ToTensor()` to an image, it converts the image into a tensor of shape `(C, H, W)`, where `C` is the number of channels (1 for grayscale images), `H` is the height, and `W` is the width. For MNIST, this means that each image will be converted to a tensor of shape `(1, 28, 28)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f8b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:11<00:00, 889kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 123kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:03<00:00, 456kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 431kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n",
      "Batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Download and create datasets\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f0937",
   "metadata": {},
   "source": [
    "## **Inspecting the Data Shape**\n",
    "\n",
    "Understanding tensor shapes is critical for debugging neural networks. Shape mismatches are the #1 cause of errors in deep learning. When you load the MNIST dataset, each image is represented as a tensor of shape `(1, 28, 28)`, where `1` is the number of channels (since MNIST images are grayscale), `28` is the height, and `28` is the width. The labels are typically represented as a tensor of shape `(N,)`, where `N` is the number of samples in the batch. For example, if you load a batch of 32 images, the image tensor will have a shape of `(32, 1, 28, 28)` and the label tensor will have a shape of `(32,)`.\n",
    "\n",
    "Let's think about what we expect:\n",
    "\n",
    "- Each image should have a shape of `(1, 28, 28)`.\n",
    "- Each label should be a single integer representing the digit class (0-9).\n",
    "- When we load a batch of images, we expect the image tensor to have a shape of `(batch_size, 1, 28, 28)` and the label tensor to have a shape of `(batch_size,)`.\n",
    "- If we see a shape that doesn't match these expectations, it could indicate an issue with how the data is being loaded or processed. For example, if the image tensor has a shape of `(28, 28)` instead of `(1, 28, 28)`, it means that the channel dimension is missing, which could lead to errors when feeding the data into a neural network.\n",
    "\n",
    "\n",
    "1. `Batch size:` We set `batch_size=64`, so each batch contains 64 images.\n",
    "2. `Image dimensions:` MNIST images are grayscale (1 channel) and are 28x28 pixels.\n",
    "3. `Shape convention:` PyTorch uses the format `(batch_size, channels, height, width)`.\n",
    "\n",
    "Therefore, the shape of a single batch should be `(64, 1, 28, 28)`.\n",
    "\n",
    "The labels will be a 1D tensor of shape `(64,)` containing the digit labels (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd77e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([64, 1, 28, 28])\n",
      "Label tensor shape: torch.Size([64])\n",
      "\n",
      "Expected image shape: (64, 1, 28, 28)\n",
      "Expected label shape: (64,)\n",
      "\n",
      "Labels in this batch: [5, 0, 8, 4, 2, 3, 8, 4, 6, 1]...\n",
      "Unique labels in y: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch of data\n",
    "X, y = next(iter(train_dataloader))\n",
    "\n",
    "\n",
    "print(\"Image tensor shape:\", X.shape)\n",
    "print(\"Label tensor shape:\", y.shape)\n",
    "print(f\"\\nExpected image shape: (64, 1, 28, 28)\")\n",
    "print(f\"Expected label shape: (64,)\")\n",
    "print(f\"\\nLabels in this batch: {y[:10].tolist()}...\")  # Show first 10 labels\n",
    "print(f\"Unique labels in y: {torch.unique(y)}\")  # Should be digits 0-9 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e78e0c6",
   "metadata": {},
   "source": [
    "## **Building the MLP Model**\n",
    "\n",
    "An MLP `(Multilayer Perceptron)` is a type of feedforward neural network that consists of multiple layers of neurons. Each layer is fully connected to the next layer. The MLP can be used for classification tasks, such as classifying the MNIST digits.\n",
    "\n",
    "### **nn.Module - The Base Class**\n",
    "\n",
    "In PyTorch, all neural network models should inherit from `nn.Module`. This base class provides a lot of functionality that makes it easier to define and train neural networks. When you create a custom model by subclassing `nn.Module`, you need to implement the `__init__` method to define the layers of your model and the `forward` method to specify how the input data flows through the layers.\n",
    "\n",
    "When you create a model, you must define two essential methods:\n",
    "\n",
    "- `__init__(self):` This method initializes the layers of the model. You define the architecture of your neural network here by creating instances of layers (e.g., `nn.Linear`, `nn.ReLU`, etc.) and assigning them as attributes of the class.\n",
    "\n",
    "- `forward(self, x):` This method defines the forward pass of the model. It specifies how the input data `x` flows through the layers defined in the `__init__` method to produce the output. The `forward` method is called when you pass data through the model (e.g., `model(input_data)`), and it should return the output of the model.\n",
    "\n",
    "\n",
    "\n",
    "### **The Layers We'll Use**\n",
    "\n",
    "- `nn.Flatten():` Converts the 2D image tensor `(1, 28, 28)` into a 1D vector `(784)`. This is necessary because fully-connected layers expect 1D input.\n",
    "\n",
    "- `nn.Linear(in_features, out_features):` A fully connected layer that applies a linear transformation to the input data. The `in_features` parameter specifies the size of each input sample, and the `out_features` parameter specifies the size of each output sample. For example, `nn.Linear(784, 128)` creates a layer that takes an input of size `784` and produces an output of size `128`.\n",
    "  \n",
    "- `nn.ReLU():` A non-linear activation function that introduces non-linearity into the model. It stands for \"Rectified Linear Unit\" and is defined as `ReLU(x) = max(0, x)`. This means that it outputs the input directly if it is positive; otherwise, it outputs zero. This helps the model learn complex patterns in the data.\n",
    "\n",
    "- `nn.Sequential(*layers):` A container module that allows you to stack layers together in a sequential manner. You can pass a list of layers to `nn.Sequential`, and it will create a single module that applies each layer in order. For example, `nn.Sequential(nn.Flatten(), nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))` creates a model that first flattens the input, then applies a linear transformation to 128 features, applies the ReLU activation, and finally applies another linear transformation to produce `10` output features (one for each digit class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd67209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Flatten 28x28 image to 784\n",
    "        self.flatten = nn.Flatten()\n",
    "        # First layer: 784 -> 128\n",
    "        self.linear1 = nn.Linear(784, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second layer: 128 -> 64\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Output layer: 64 -> 10 (one for each digit 0-9)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleMLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ba0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SequentialMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequentialMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),          # Flatten 28x28 to 784\n",
    "            nn.Linear(784, 128),  # First layer\n",
    "            nn.ReLU(),            # Activation\n",
    "            nn.Linear(128, 64),   # Second layer\n",
    "            nn.ReLU(),            # Activation\n",
    "            nn.Linear(64, 10)     # Output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the Sequential model\n",
    "sequential_model = SequentialMLP()\n",
    "print(sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a6835",
   "metadata": {},
   "source": [
    "## **The Training Essentials**\n",
    "\n",
    "To train a neural network, you need to define three key components:\n",
    "\n",
    "1. **Loss Function:** The loss function measures how well the model's predictions match the true labels. For classification tasks, a common loss function is `nn.CrossEntropyLoss()`, which combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    "   \n",
    "2. **Optimizer:** The optimizer updates the model's parameters based on the computed gradients. A common choice is `torch.optim.SGD` (Stochastic Gradient Descent) or `torch.optim.Adam`, which is an adaptive learning rate optimization algorithm.\n",
    "   \n",
    "3. **Training Loop:** The training loop iterates over the dataset for a specified number of epochs. In each epoch, you loop through the batches of data, perform a forward pass to compute the predictions, compute the loss, perform a backward pass to compute the gradients, and then update the model's parameters using the optimizer.\n",
    "\n",
    "    - Feeding data to the model\n",
    "    - Calculating the loss\n",
    "    - Performing backpropagation\n",
    "    - Updating the model's parameters\n",
    "    - repeating the process for multiple epochs until the model converges or reaches a satisfactory level of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd59f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate optimizer\n",
    "# lr = learning rate (how big of steps to take)\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Loss function:\", loss_fn)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c4095",
   "metadata": {},
   "source": [
    "## **The Training Loop Explained** \n",
    "\n",
    "In the training loop, we iterate over the training data for a specified number of epochs. For each batch of data, we perform the following steps:\n",
    "1. **Forward Pass:** We pass the input data `X` through the model to get the predictions `y_pred`. This is done by calling `model(X)`, which internally calls the `forward` method of the model.\n",
    "\n",
    "2. **Calculate Loss:** We compute the loss by comparing the predicted labels `y_pred` with the true labels `y` using the loss function defined earlier (e.g., `loss_fn(y_pred, y)`).\n",
    "\n",
    "3. **Backward Pass:** We call `loss.backward()` to compute the gradients of the loss with respect to the model's parameters. This populates the `.grad` attributes of the parameters with the computed gradients.\n",
    "\n",
    "4. **Update Parameters:** We call `optimizer.step()` to update the model's parameters based on the computed gradients. This step modifies the parameters in the direction that reduces the loss.\n",
    "\n",
    "5. **Zero Gradients:** We call `optimizer.zero_grad()` to reset the gradients of the model's parameters to zero. This is important because, by default, PyTorch accumulates gradients, so we need to clear them before the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf28a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/5, Batch 100/938, Loss: 0.0464\n",
      "Epoch 1/5, Batch 200/938, Loss: 0.0418\n",
      "Epoch 1/5, Batch 300/938, Loss: 0.0414\n",
      "Epoch 1/5, Batch 400/938, Loss: 0.0417\n",
      "Epoch 1/5, Batch 500/938, Loss: 0.0427\n",
      "Epoch 1/5, Batch 600/938, Loss: 0.0430\n",
      "Epoch 1/5, Batch 700/938, Loss: 0.0435\n",
      "Epoch 1/5, Batch 800/938, Loss: 0.0439\n",
      "Epoch 1/5, Batch 900/938, Loss: 0.0450\n",
      "Epoch 1/5 completed. Average Loss: 0.0455\n",
      "\n",
      "Epoch 2/5, Batch 100/938, Loss: 0.0331\n",
      "Epoch 2/5, Batch 200/938, Loss: 0.0320\n",
      "Epoch 2/5, Batch 300/938, Loss: 0.0340\n",
      "Epoch 2/5, Batch 400/938, Loss: 0.0338\n",
      "Epoch 2/5, Batch 500/938, Loss: 0.0331\n",
      "Epoch 2/5, Batch 600/938, Loss: 0.0332\n",
      "Epoch 2/5, Batch 700/938, Loss: 0.0324\n",
      "Epoch 2/5, Batch 800/938, Loss: 0.0327\n",
      "Epoch 2/5, Batch 900/938, Loss: 0.0347\n",
      "Epoch 2/5 completed. Average Loss: 0.0353\n",
      "\n",
      "Epoch 3/5, Batch 100/938, Loss: 0.0245\n",
      "Epoch 3/5, Batch 200/938, Loss: 0.0242\n",
      "Epoch 3/5, Batch 300/938, Loss: 0.0251\n",
      "Epoch 3/5, Batch 400/938, Loss: 0.0272\n",
      "Epoch 3/5, Batch 500/938, Loss: 0.0271\n",
      "Epoch 3/5, Batch 600/938, Loss: 0.0279\n",
      "Epoch 3/5, Batch 700/938, Loss: 0.0283\n",
      "Epoch 3/5, Batch 800/938, Loss: 0.0292\n",
      "Epoch 3/5, Batch 900/938, Loss: 0.0291\n",
      "Epoch 3/5 completed. Average Loss: 0.0291\n",
      "\n",
      "Epoch 4/5, Batch 100/938, Loss: 0.0225\n",
      "Epoch 4/5, Batch 200/938, Loss: 0.0208\n",
      "Epoch 4/5, Batch 300/938, Loss: 0.0214\n",
      "Epoch 4/5, Batch 400/938, Loss: 0.0208\n",
      "Epoch 4/5, Batch 500/938, Loss: 0.0219\n",
      "Epoch 4/5, Batch 600/938, Loss: 0.0223\n",
      "Epoch 4/5, Batch 700/938, Loss: 0.0238\n",
      "Epoch 4/5, Batch 800/938, Loss: 0.0240\n",
      "Epoch 4/5, Batch 900/938, Loss: 0.0241\n",
      "Epoch 4/5 completed. Average Loss: 0.0242\n",
      "\n",
      "Epoch 5/5, Batch 100/938, Loss: 0.0141\n",
      "Epoch 5/5, Batch 200/938, Loss: 0.0168\n",
      "Epoch 5/5, Batch 300/938, Loss: 0.0180\n",
      "Epoch 5/5, Batch 400/938, Loss: 0.0192\n",
      "Epoch 5/5, Batch 500/938, Loss: 0.0194\n",
      "Epoch 5/5, Batch 600/938, Loss: 0.0214\n",
      "Epoch 5/5, Batch 700/938, Loss: 0.0214\n",
      "Epoch 5/5, Batch 800/938, Loss: 0.0220\n",
      "Epoch 5/5, Batch 900/938, Loss: 0.0220\n",
      "Epoch 5/5 completed. Average Loss: 0.0222\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epochs: int):\n",
    "    \"\"\"\n",
    "    Train the model for a specified number of epochs.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: DataLoader providing batches of training data\n",
    "        model: The neural network model\n",
    "        loss_fn: Loss function\n",
    "        optimizer: Optimizer for updating weights\n",
    "        epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (X, y) in enumerate(dataloader):\n",
    "            # Step 1: Forward pass\n",
    "            pred = model(X)\n",
    "            \n",
    "            # Step 2: Calculate loss\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # Step 3: Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Step 4: Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Step 5: Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Print average loss for the epoch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f'Epoch {epoch + 1}/{epochs} completed. Average Loss: {avg_loss:.4f}\\n')\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "print(\"Starting training...\\n\")\n",
    "train(train_dataloader, model, loss_fn, optimizer, epochs=epochs)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad19c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-llm-majors (3.10.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
