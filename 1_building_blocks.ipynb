{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b87304",
   "metadata": {},
   "source": [
    "## **1: The Building Blocks - Environment & PyTorch Tensors**\n",
    "\n",
    "In this notebook, we'll set up a clean, modern Python environment and master the fundamental data object in all of deep learning: the tensor. Understanding tensors is crucialâ€”they are the foundation upon which all neural network operations are built.\n",
    "\n",
    "### **Platform Disclaimer**\n",
    "\n",
    "**Important:** This tutorial is designed for macOS, Linux, and Windows. The tooling (uv) is optimized for macOS and Linux. For GPU acceleration, PyTorch supports:\n",
    "\n",
    "- `CUDA` (NVIDIA GPUs on Linux/Windows)\n",
    "- `MPS` (Apple Silicon GPUs on macOS)\n",
    "- `CPU` (fallback for all platforms)\n",
    "\n",
    "The code will automatically detect and use the best available device.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Setting Up the Environment**\n",
    "\n",
    "We'll use `uv` to create a clean Python environment. This ensures that our dependencies are isolated and won't conflict with other projects.\n",
    "\n",
    "```bash\n",
    "# Create a new environment named 'dl-llm-majors'\n",
    "uv new dl-llm-majors\n",
    "# Activate the environment\n",
    "uv activate dl-llm-majors\n",
    "```\n",
    "\n",
    "\n",
    "### **2. Installing PyTorch**\n",
    "\n",
    "Next, we'll install PyTorch. The installation command varies based on your platform and desired features (like GPU support). Visit the [PyTorch Get Started](https://pytorch.org/get-started/locally/) page to find the right command for your setup. For example, for CUDA support on Linux, you might use:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "\n",
    "### **3. Verifying the Installation**\n",
    "\n",
    "To verify that PyTorch is installed correctly and to check if it can access the GPU, run the following code in a Python shell:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "```\n",
    "\n",
    "This will confirm that PyTorch is installed and whether it can utilize your GPU.\n",
    "\n",
    "\n",
    "\n",
    "### **Understanding Tensors**\n",
    "\n",
    "Tensors are multi-dimensional arrays that are the core data structure in PyTorch. They can be manipulated on both CPUs and GPUs, making them essential for deep learning. Here's how to create and manipulate tensors:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"Tensor x:\\n\", x)\n",
    "\n",
    "# Perform operations on tensors\n",
    "y = x + 10\n",
    "print(\"Tensor y (x + 10):\\n\", y)\n",
    "z = x * 2\n",
    "print(\"Tensor z (x * 2):\\n\", z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82c5a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74594614",
   "metadata": {},
   "source": [
    "## **Introduction to Tensors**\n",
    "\n",
    "- What is a tensor? In the simplest terms: It's a multi-dimensional array, like a list of lists of lists. Think of it as a super-powered NumPy array that can run on GPUs.\n",
    "- Tensors can be 0D (scalar), 1D (vector), 2D (matrix), or even higher-dimensional. They are the fundamental building blocks for all operations in deep learning, from simple arithmetic to complex neural network computations.\n",
    "\n",
    "Tensors are the backbone of deep learning frameworks like PyTorch. They allow us to perform efficient computations on large datasets, and they can be easily moved between CPUs and GPUs for faster processing. Understanding how to create, manipulate, and utilize tensors is essential for anyone looking to dive into deep learning. We can create tensors from\n",
    "\n",
    "- Python lists, \n",
    "- NumPy arrays,\n",
    "- Using built-in functions like `torch.rand()`, `torch.ones()`, `torch.zeros()`,\n",
    "- With specific values using `torch.tensor()` \n",
    "- or even directly from data files. \n",
    "\n",
    "Once we have our tensors, we can perform a wide range of operations on them, such as addition, multiplication, and more complex functions like matrix multiplication and convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c0b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from list: tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create tensor from a Python list\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"Tensor from list:\", tensor_from_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a42e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from NumPy array:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Create tensor from a NumPy array\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(\"Tensor from NumPy array:\\n\", tensor_from_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6289b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor:\n",
      " tensor([[0.8013, 0.0548, 0.3567],\n",
      "        [0.1011, 0.1140, 0.5162],\n",
      "        [0.4225, 0.4603, 0.3205]])\n"
     ]
    }
   ],
   "source": [
    "# Create random tensor\n",
    "random_tensor = torch.rand(3, 3)\n",
    "print(\"Random tensor:\\n\", random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e672ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of ones:\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create tensor of ones\n",
    "ones_tensor = torch.ones(2, 4)\n",
    "print(\"Tensor of ones:\\n\", ones_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db93c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of zeros:\n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create tensor of zeros\n",
    "zeros_tensor = torch.zeros(2, 4)\n",
    "print(\"Tensor of zeros:\\n\", zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58717dbe",
   "metadata": {},
   "source": [
    "## **Tensor Attributes**\n",
    "\n",
    "Every tensor has attributes that describe its properties:\n",
    "- `shape`: The dimensions of the tensor (e.g., (2, 3) for a 2D tensor with 2 rows and 3 columns).\n",
    "- `dtype`: The data type of the tensor (e.g., `torch.float32`, `torch.int64`).\n",
    "- `device`: The device on which the tensor is stored (e.g., `cpu`, `cuda`, `mps`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45361020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Shape: torch.Size([2, 2])\n",
      "Data type: torch.float32\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor and inspect its attributes\n",
    "tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, device='cpu')\n",
    "\n",
    "print(\"Tensor:\\n\", tensor)\n",
    "print(\"Shape:\", tensor.shape)\n",
    "print(\"Data type:\", tensor.dtype)\n",
    "print(\"Device:\", tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41e625",
   "metadata": {},
   "source": [
    "## **The Art of Shaping Data**\n",
    "\n",
    "Tensor operations are the core skill for a code-first deep learning practitioner. To perform operations on tensors, they often need to be the same shape. This is where tensor manipulation comes in. You can reshape, permute, and broadcast tensors to make them compatible for operations. For example, if you have a tensor of shape (2, 3) and another of shape (3,), you can reshape the second tensor to (1, 3) and then perform operations like addition or multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aa3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 4x4 tensor\n",
    "matrix = torch.arange(16).reshape(4, 4)\n",
    "print(\"Original matrix:\\n\", matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad83c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:\n",
      " tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Slice the first row\n",
    "first_row = matrix[0, :]\n",
    "print(\"First row:\\n\", first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "725e5fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First column:\n",
      " tensor([ 0,  4,  8, 12])\n"
     ]
    }
   ],
   "source": [
    "# Slice the first column\n",
    "first_column = matrix[:, 0]\n",
    "print(\"First column:\\n\", first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10f888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced matrix (first two rows):\n",
      " tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Slice the matrix to get the first two rows\n",
    "sliced_matrix = matrix[:2, :]\n",
    "print(\"Sliced matrix (first two rows):\\n\", sliced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716de565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced matrix (first two columns):\n",
      " tensor([[ 0,  1],\n",
      "        [ 4,  5],\n",
      "        [ 8,  9],\n",
      "        [12, 13]])\n"
     ]
    }
   ],
   "source": [
    "# Slice the matrix to get the first two columns\n",
    "sliced_matrix_columns = matrix[:, :2]\n",
    "print(\"Sliced matrix (first two columns):\\n\", sliced_matrix_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5001b8",
   "metadata": {},
   "source": [
    "## **Reshaping with View**\n",
    "\n",
    "The `view()` method allows you to reshape a tensor without changing its data. For example, if you have a tensor of shape `(4, 4)` and you want to reshape it to `(2, 8)`, you can do so with `view()`.\n",
    "\n",
    "The `-1` argument in `view()` is a special placeholder that tells PyTorch to infer the size of that dimension based on the total number of elements and the other specified dimensions. For example, if you have a tensor with 16 elements and you want to reshape it to have 4 rows, you can use `view(4, -1)`, and PyTorch will automatically calculate that the number of columns should be 4 (since 16 / 4 = 4). This makes it easy to reshape tensors without having to manually calculate the new dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4592ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Original shape: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with initial shape\n",
    "original_tensor = torch.arange(12)\n",
    "print(\"Original tensor:\", original_tensor)\n",
    "print(\"Original shape:\", original_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89091ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor:\n",
      " tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "Reshaped shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Reshape using view with -1 trick\n",
    "reshaped_tensor = original_tensor.view(4, -1)\n",
    "print(\"Reshaped tensor:\\n\", reshaped_tensor)\n",
    "print(\"Reshaped shape:\", reshaped_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f915918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor (3, 4):\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the tensor to (3, 4)\n",
    "reshaped_tensor = original_tensor.view(3, 4)\n",
    "print(\"Reshaped tensor (3, 4):\\n\", reshaped_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109f641",
   "metadata": {},
   "source": [
    "## **Adding and Removing Dimensions**\n",
    "\n",
    "You can add dimensions to a tensor using `unsqueeze()` and remove them with `squeeze()`. For example, if you have a tensor of shape (3,) and you want to add a new dimension to make it (1, 3), you can use `unsqueeze(0)`. Conversely, if you have a tensor of shape (1, 3) and you want to remove the extra dimension, you can use `squeeze(0)` to get back to (3,).\n",
    "\n",
    "\n",
    "- `unsqueeze(dim)`: Adds a dimension of size 1 at the specified position `dim`. For example, if you have a tensor of shape (3,) and you call `unsqueeze(0)`, it will become (1, 3). If you call `unsqueeze(1)`, it will become (3, 1).\n",
    "- `squeeze(dim)`: Removes a dimension of size 1 at the specified position `dim`. For example, if you have a tensor of shape (1, 3) and you call `squeeze(0)`, it will become (3,). If you have a tensor of shape (3, 1) and you call `squeeze(1)`, it will also become (3,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c88f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([1, 2, 3, 4, 5])\n",
      "Original shape: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"Original tensor:\", tensor)\n",
    "print(\"Original shape:\", tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a053bcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor after unsqueeze(0):\n",
      " tensor([[1, 2, 3, 4, 5]])\n",
      "Shape after unsqueeze(0): torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Add a dimension at position 0 (adds batch dimension)\n",
    "unsqueezed_tensor = tensor.unsqueeze(0)\n",
    "print(\"Tensor after unsqueeze(0):\\n\", unsqueezed_tensor)\n",
    "print(\"Shape after unsqueeze(0):\", unsqueezed_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23783dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor after squeeze(0):\n",
      " tensor([1, 2, 3, 4, 5])\n",
      "Shape after squeeze(0): torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# Remove the added dimension\n",
    "squeezed_tensor = unsqueezed_tensor.squeeze(0)\n",
    "print(\"Tensor after squeeze(0):\\n\", squeezed_tensor)\n",
    "print(\"Shape after squeeze(0):\", squeezed_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c5de92",
   "metadata": {},
   "source": [
    "## **GPU vs. CPU Acceleration**\n",
    "\n",
    "PyTorch allows you to perform tensor operations on both CPUs and GPUs. If you have a compatible GPU, you can move your tensors to the GPU for faster computation. This is done using the `to()` method. For example, if you have a tensor `x` and you want to move it to the GPU, you can use `x.to('cuda')`. If you're on a Mac with an M1 chip, you can use `x.to('mps')` to take advantage of the Apple Silicon GPU. If you don't have a compatible GPU, PyTorch will automatically fall back to using the CPU, so your code will still run, albeit more slowly. It's important to check for GPU availability and move your tensors accordingly to maximize performance.\n",
    "\n",
    "- `to(device)`: Moves the tensor to the specified device. For example, `x.to('cuda')` moves the tensor to the GPU, while `x.to('cpu')` moves it back to the CPU. You can also use `x.to('mps')` for Apple Silicon GPUs on macOS.\n",
    "- `CUDA` (NVIDIA GPUs on Linux/Windows)\n",
    "- `MPS` (Apple Silicon GPUs on macOS)\n",
    "- `CPU` (fallback for all platforms)\n",
    "\n",
    "In the next cell, we'll:\n",
    "\n",
    "- Detect the best available device (CUDA for NVIDIA GPUs, MPS for Apple Silicon, or CPU as a fallback).\n",
    "- Create two large tensors and perform a simple operation on them to demonstrate the speed difference between CPU and GPU computations.\n",
    "- Time the operations to show the performance benefits of using a GPU when available.\n",
    "- Move them to the GPU (if available) and perform the same operation to see the speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc3edf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU) for computations.\n",
      "CPU computation time: 0.0239 seconds\n",
      "MPS computation time: 0.0003 seconds\n",
      "Speedup: 70.69x faster than CPU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Detect the best available device: CUDA -> MPS (Apple Silicon) -> CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (GPU) for computations.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU) for computations.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for computations.\")\n",
    "\n",
    "\n",
    "size = 2000\n",
    "a_cpu = torch.rand(size, size)\n",
    "b_cpu = torch.rand(size, size)\n",
    "\n",
    "# Time CPU computation\n",
    "start_time = time.time()\n",
    "result_cpu = torch.mm(a_cpu, b_cpu)\n",
    "cpu_time = time.time() - start_time\n",
    "print(f\"CPU computation time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Time GPU computation (if available)\n",
    "if device.type != 'cpu':\n",
    "    a_gpu = a_cpu.to(device)\n",
    "    b_gpu = b_cpu.to(device)\n",
    "    \n",
    "    # Warm up (first operation can be slower due to initialization)\n",
    "    _ = torch.mm(a_gpu, b_gpu)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result_gpu = torch.mm(a_gpu, b_gpu)\n",
    "    gpu_time = time.time() - start_time\n",
    "    print(f\"{device.type.upper()} computation time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time / gpu_time:.2f}x faster than CPU\")\n",
    "else:\n",
    "    print(\"GPU not available, skipping GPU computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f4542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-llm-majors (3.10.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
